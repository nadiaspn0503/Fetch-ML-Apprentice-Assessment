{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch ML Apprentice Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the appropriate libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the sentence transformer model\n",
    "\n",
    "class SentenceTransformerModel(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', pooling='mean'):\n",
    "        super(SentenceTransformerModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        encoded_input = self.tokenizer(\n",
    "            sentences, padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad():  # No training\n",
    "            model_output = self.bert(**encoded_input)\n",
    "\n",
    "        # Token embeddings (batch_size, seq_len, hidden_size)\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "\n",
    "        # Attention mask for ignoring padding tokens\n",
    "        attention_mask = encoded_input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size())\n",
    "\n",
    "        if self.pooling == 'mean':\n",
    "            # Mean Pooling: sum embeddings then divide by number of valid tokens\n",
    "            summed = torch.sum(token_embeddings * attention_mask, dim=1)\n",
    "            summed_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n",
    "            sentence_embeddings = summed / summed_mask\n",
    "        elif self.pooling == 'cls':\n",
    "            # CLS Pooling: use first token\n",
    "            sentence_embeddings = token_embeddings[:, 0]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pooling type\")\n",
    "\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768])\n",
      "tensor([[ 0.5121,  0.1326, -0.0188,  ..., -0.2387, -0.0629,  0.2001],\n",
      "        [ 0.4206, -0.0512,  0.3857,  ..., -0.1994,  0.1497, -0.0953],\n",
      "        [ 0.0431, -0.6125,  0.2400,  ..., -0.1788, -0.1408, -0.0080],\n",
      "        [ 0.3266,  0.0353,  0.0498,  ..., -0.1261, -0.1274,  0.0138],\n",
      "        [ 0.2340, -0.3160, -0.3238,  ..., -0.0248,  0.2804,  0.0624]])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = SentenceTransformerModel(pooling='mean')\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love saving money with Fetch.\",\n",
    "    \"I would be a great ML Apprentice.\",\n",
    "    \"Fetch Rewards is a useful app.\",\n",
    "    \"I enjoy creating ML models.\",\n",
    "    \"My favorite color is blue.\"\n",
    "]\n",
    "\n",
    "# Encode and print embeddings\n",
    "embeddings = model(sentences)\n",
    "print(embeddings.shape)  # Should be [3, 768] for BERT base\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation for the model architecture outside of the transformer backbone.\n",
    "\n",
    "Mean pooling was used to encode input sentences into fixed-length embeddings. This is because mean pooling is simple yet effective. It works well for NLP as it reduces dimensionality and prevents overfitting. To keep things simple, no extra layers were added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the shared BERT encoder\n",
    "\n",
    "class SharedBERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(SharedBERTEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # Tokenize and encode sentences\n",
    "        encoded_input = self.tokenizer(\n",
    "            sentences, padding=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get BERT model output (we don't fine-tune BERT in this case)\n",
    "        with torch.no_grad():  # No training on BERT backbone\n",
    "            model_output = self.bert(**encoded_input)\n",
    "        \n",
    "        return model_output.last_hidden_state, encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.A: Sentence Classification \n",
    "\n",
    "class TaskASentenceClassification(nn.Module):\n",
    "    def __init__(self, bert_encoder, num_classes=3):\n",
    "        super(TaskASentenceClassification, self).__init__()\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.task_a_fc = nn.Linear(self.bert_encoder.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # Get sentence embeddings from shared encoder\n",
    "        token_embeddings, attention_mask = self.bert_encoder(sentences)\n",
    "        \n",
    "        # We use mean pooling here\n",
    "        summed = torch.sum(token_embeddings * attention_mask.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        # Ensure summed_mask has the correct shape (batch_size, 1)\n",
    "        summed_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-9).unsqueeze(-1)  # (batch_size, 1)\n",
    "        sentence_embeddings = summed / summed_mask  # Now the division works element-wise\n",
    "        \n",
    "        # Task A output: classification logits\n",
    "        task_a_output = self.task_a_fc(sentence_embeddings)\n",
    "        return task_a_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A Output (Sentence Classification):\n",
      "Sentence: 'I love saving money with Fetch.'\n",
      "Predicted probabilities: [0.3767282  0.30354527 0.3197265 ]\n",
      "Sentence: 'I would be a great ML Apprentice.'\n",
      "Predicted probabilities: [0.40083712 0.3210542  0.27810866]\n",
      "Sentence: 'Fetch Rewards is a useful app.'\n",
      "Predicted probabilities: [0.30036512 0.34459034 0.35504457]\n",
      "Sentence: 'I enjoy creating ML models.'\n",
      "Predicted probabilities: [0.39686894 0.31306687 0.29006413]\n",
      "Sentence: 'My favorite color is blue.'\n",
      "Predicted probabilities: [0.31644395 0.27996066 0.40359542]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the shared encoder (BERT)\n",
    "shared_bert_encoder = SharedBERTEncoder().to(device)\n",
    "\n",
    "# Initialize Task A (Sentence Classification)\n",
    "task_a_model = TaskASentenceClassification(bert_encoder=shared_bert_encoder, num_classes=3).to(device)\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I love saving money with Fetch.\", \n",
    "    \"I would be a great ML Apprentice.\",\n",
    "    \"Fetch Rewards is a useful app.\", \n",
    "    \"I enjoy creating ML models.\", \n",
    "    \"My favorite color is blue.\"\n",
    "]\n",
    "\n",
    "# Get predictions for Task A (Sentence Classification)\n",
    "task_a_output = task_a_model(sentences)\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "softmax = nn.Softmax(dim=1)\n",
    "task_a_probs = softmax(task_a_output)\n",
    "\n",
    "# Print the output probabilities for each class\n",
    "print(\"Task A Output (Sentence Classification):\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(f\"Predicted probabilities: {task_a_probs[i].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.B: Sentiment Analysis\n",
    "\n",
    "# Task B (Sentiment Analysis)\n",
    "class TaskBSentimentAnalysis(nn.Module):\n",
    "    def __init__(self, bert_encoder, num_classes=3):  # Positive, Neutral, Negative\n",
    "        super(TaskBSentimentAnalysis, self).__init__()\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.task_b_fc = nn.Linear(self.bert_encoder.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        token_embeddings, attention_mask = self.bert_encoder(sentences)\n",
    "        summed = torch.sum(token_embeddings * attention_mask.unsqueeze(-1), dim=1)\n",
    "        summed_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-9).unsqueeze(-1)\n",
    "        sentence_embeddings = summed / summed_mask\n",
    "        task_b_output = self.task_b_fc(sentence_embeddings)\n",
    "        return task_b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task B Output (Sentiment Analysis):\n",
      "Sentence: 'I love saving money with Fetch.'\n",
      "Predicted sentiment: Neutral\n",
      "Probabilities: [0.29477343 0.44812414 0.2571024 ]\n",
      "--------------------------------------------------\n",
      "Sentence: 'I would be a great ML Apprentice.'\n",
      "Predicted sentiment: Neutral\n",
      "Probabilities: [0.35747185 0.39837918 0.24414898]\n",
      "--------------------------------------------------\n",
      "Sentence: 'Fetch Rewards is a useful app.'\n",
      "Predicted sentiment: Neutral\n",
      "Probabilities: [0.32055002 0.4254784  0.25397152]\n",
      "--------------------------------------------------\n",
      "Sentence: 'I enjoy creating ML models.'\n",
      "Predicted sentiment: Neutral\n",
      "Probabilities: [0.36590302 0.39256594 0.24153106]\n",
      "--------------------------------------------------\n",
      "Sentence: 'My favorite color is blue.'\n",
      "Predicted sentiment: Neutral\n",
      "Probabilities: [0.3419062  0.4364615  0.22163229]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "\n",
    "# Initialize Task B\n",
    "task_b_model = TaskBSentimentAnalysis(bert_encoder=shared_bert_encoder, num_classes=3).to(device)\n",
    "\n",
    "# Get predictions for Task B (Sentiment Analysis)\n",
    "task_b_output = task_b_model(sentences)\n",
    "task_b_probs = softmax(task_b_output)\n",
    "\n",
    "# Define sentiment class names\n",
    "sentiment_classes = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "# Print results\n",
    "print(\"Task B Output (Sentiment Analysis):\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    predicted_class = sentiment_classes[task_b_probs[i].argmax(dim=0).item()]\n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(f\"Predicted sentiment: {predicted_class}\")\n",
    "    print(f\"Probabilities: {task_b_probs[i].detach().cpu().numpy()}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes made to the architecture to support multi-task learning.\n",
    "\n",
    "The shared BERT encoder allows me to complete both tasks without recreating the same model. It also allows me to be able to add more tasks later if need be. \n",
    "\n",
    "In Task A I created a fully connected linear layer that uses the BERT vector to output the probability score for predefined sentence classes. These classes include personal statements, general facts, and app-related sentences.\n",
    "\n",
    "In Task B I used the same BERT vector and created another fully connected linear layer that outputs the probability score for sentiment classes. These classes include positive, negative, and neutral sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Training Considerations\n",
    "### Implications and advantages of scenarios and the rationale as to how the model should be trained\n",
    "1. The entire network is frozen:\n",
    "   - Implications and Advantages: All parameters like the transformer and task-specific heads are frozen. The model is then used as a static encoder. This is useful for feature extraction as training is fast and no gradient updates are required. Although it is rare, it would be useful if the pre-trained model performs significantly well on the tasks. This also would be less computationally expensive.\n",
    "   - How the model should be trained: Use the transformer to extract embeddings (I used mean pooling). Pass the embeddings through a separate classifier and train it independently.\n",
    "2. Only the transformer backbone should be frozen:\n",
    "   - Implications and Advantages: The transformer stays stable which reduces overfitting on small datasets. Only task-specific classification heads are trainable; however, they are fast and easy to train. This would also preserve the linguistic knowledge that is captured during pre-training. \n",
    "   - How the model should be trained: Freeze the transformer parameters then use a loss function (I used cross-entropy) to define and train task-specific heads. Update the head parameters with a standard training loop.\n",
    "3. Only one of the task-specific heads (either for Task A or Task B) should be frozen\n",
    "   - Implications and Advantages: One task (Task A: Classification) is stable while the other task (Task B: Sentiment) is trained. This is useful for preventing catastrophic forgetting as it allows one task to improve without degrading the performance of the other task that performs well. The shared backbone is also updated. \n",
    "   - How the model should be trained: Freeze the well-performing task then update the head of the other task. Calculate the loss for only the updated task.\n",
    "\n",
    "### Approaching the transfer learning process\n",
    "The choice for the pre-trained model is the BERT-base transformer. This is because it is pre-trained on massive corpora which reduces the computational price and the need for large labeled datasets. It also provides a strong general language understanding and generates sufficient sentence embeddings. It even transfers well across tasks as it is widely supported and well-documented.\n",
    "Initially, The embedding and transformer encoder layers are frozen. Train the task-specific heads. As training goes on, specifically mid-training, gradually unfreeze a few of the top layers of BERT so they may adapt to task-specific signals and improve performance. This also avoids catastrophic forgetting and reduces training time. Lastly, the entire model should be tuned with a low learning rate. During tuning, all BERT layers can be unfrozen. This allows the model to learn domain-specific context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Training Loop Implentation (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A Loss: 1.1074, Accuracy: 0.40\n",
      "Task B Loss: 1.1614, Accuracy: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/_compile.py:51: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Hypothetical batch of input sentences\n",
    "sentences = [\n",
    "    \"I love saving money with Fetch.\",\n",
    "    \"I would be a great ML Apprentice.\",\n",
    "    \"Fetch Rewards is a useful app.\",\n",
    "    \"I enjoy creating ML models.\",\n",
    "    \"My favorite color is blue.\"\n",
    "]\n",
    "\n",
    "# Dummy labels\n",
    "task_a_labels = torch.tensor([0, 1, 0, 1, 2])  # Sentence classification (App, Statement, Fact)\n",
    "task_b_labels = torch.tensor([2, 2, 2, 2, 1])  # Sentiment (Positive, Positive, Positive, Positive, Neutral)\n",
    "\n",
    "# Move labels to device\n",
    "task_a_labels = task_a_labels.to(device)\n",
    "task_b_labels = task_b_labels.to(device)\n",
    "\n",
    "# Initialize models\n",
    "shared_encoder = SharedBERTEncoder().to(device)\n",
    "task_a_model = TaskASentenceClassification(shared_encoder, num_classes=3).to(device)\n",
    "task_b_model = TaskBSentimentAnalysis(shared_encoder, num_classes=3).to(device)\n",
    "\n",
    "# Loss functions and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(task_a_model.parameters()) + list(task_b_model.parameters()), lr=2e-5)\n",
    "\n",
    "# === Training Step (Single Batch Example) ===\n",
    "task_a_model.train()\n",
    "task_b_model.train()\n",
    "\n",
    "# Zero gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward passes\n",
    "task_a_logits = task_a_model(sentences)           # (batch_size, num_classes)\n",
    "task_b_logits = task_b_model(sentences)           # (batch_size, num_classes)\n",
    "\n",
    "# Compute losses\n",
    "task_a_loss = criterion(task_a_logits, task_a_labels)\n",
    "task_b_loss = criterion(task_b_logits, task_b_labels)\n",
    "\n",
    "# Combine losses (equal weight)\n",
    "total_loss = task_a_loss + task_b_loss\n",
    "\n",
    "# Backpropagation\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Compute accuracy\n",
    "task_a_preds = torch.argmax(task_a_logits, dim=1)\n",
    "task_b_preds = torch.argmax(task_b_logits, dim=1)\n",
    "\n",
    "task_a_acc = (task_a_preds == task_a_labels).float().mean()\n",
    "task_b_acc = (task_b_preds == task_b_labels).float().mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Task A Loss: {task_a_loss.item():.4f}, Accuracy: {task_a_acc.item():.2f}\")\n",
    "print(f\"Task B Loss: {task_b_loss.item():.4f}, Accuracy: {task_b_acc.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions and Decisions\n",
    "\n",
    "The sentences are embedded through a shared BERT encoder to reduce model size, training time, and computational cost. Since each task has different output requirements, I created two separate task-specific heads. Dummy labels were created as hypothetical data to focus on the architecture of the model. Assuming the losses for each task are comparable in class, I used cross-entropy loss as the loss function for both tasks. Accuracy is used as an evaluation metric as this simple metric works well at tracking classification performance. Only one batch was used for simplicity; however, in practice, multiple batches would be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
